{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/kanishkjain/opt/anaconda3/envs/gym/lib/python3.9/site-packages\")\n",
    "\n",
    "import random\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "import gym\n",
    "import gym_toytext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, environment=\"Roulette-v0\", gamma=0.1, theta=1e-6, epsilon=1.0, alpha=0.1\n",
    "    ) -> None:\n",
    "\n",
    "        self.env = gym.make(environment)\n",
    "        self.env.reset()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.A_space = self.env.action_space\n",
    "        self.S_space = self.env.observation_space\n",
    "        self.R_range = self.env.reward_range\n",
    "\n",
    "        self.Num_A = self.A_space.n\n",
    "        self.Num_S = self.S_space.n\n",
    "        \n",
    "        if environment == 'CliffWalking-v0':\n",
    "            for s in range(self.Num_S):\n",
    "                for a in range(self.Num_A):\n",
    "                    P, S_, R_, T = self.env.P[s][a][0]\n",
    "                    if T:\n",
    "                        self.env.P[s][a] = [(P, S_, 0, T)]\n",
    "\n",
    "    def soft_policy(self):\n",
    "\n",
    "        Pi = np.ones((self.Num_S, self.Num_A)) / self.Num_A\n",
    "        return Pi\n",
    "\n",
    "    def greedy_policy(self, Q):\n",
    "        Pi = np.zeros((self.Num_S, self.Num_A))\n",
    "        for s in range(self.Num_S):\n",
    "            a_star = np.argmax([Q[(s, a)] for a in range(self.Num_A)])\n",
    "            Pi[s, a_star] = 1.0\n",
    "        return Pi\n",
    "\n",
    "    def epsilon_greedy_policy(self, Q, s):\n",
    "\n",
    "        p = random.random()\n",
    "        if p < self.epsilon:\n",
    "            return np.random.choice(self.Num_A)\n",
    "        else:\n",
    "            # A = np.argmax([Q[(s, a)] for a in range(self.Num_A)])\n",
    "            # prob = np.array([np.exp(x) for x in Q[s]])\n",
    "            # prob = prob/(sum(prob))\n",
    "            # A = np.random.choice(self.Num_A, p=prob)\n",
    "            A = np.argmax(Q[s])\n",
    "            return A\n",
    "\n",
    "    def on_policy_monte_carlo(self, num_iter=10):\n",
    "\n",
    "        self.epsilon = 0.9\n",
    "\n",
    "        NUM_ITER = num_iter\n",
    "\n",
    "        Q = collections.defaultdict(lambda: np.ones(self.Num_A)/self.Num_A)\n",
    "        returns = collections.defaultdict(float)\n",
    "\n",
    "        Pi = self.soft_policy()\n",
    "        print(\"Starting Policy:, \", Pi)\n",
    "\n",
    "        S_A_count = collections.defaultdict(int)\n",
    "\n",
    "        rewards_per_episode = []\n",
    "        unique_states = []\n",
    "\n",
    "        for it in range(NUM_ITER):\n",
    "            episode = self.generate_episode(Pi)\n",
    "            if it % 50 == 0:\n",
    "                print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "            # self.epsilon = max(self.epsilon * 0.99, 0.01)\n",
    "\n",
    "            S_A = set([(S, A) for (S, A, _) in episode])\n",
    "\n",
    "            for S, A in S_A:\n",
    "                first_idx = [\n",
    "                    i for i, (s, a, _) in enumerate(episode) if (s == S and a == A)\n",
    "                ][0]\n",
    "                G = sum(\n",
    "                    [\n",
    "                        r * (self.gamma ** i)\n",
    "                        for i, (s, a, r) in enumerate(episode[first_idx:])\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                returns[(S, A)] += G\n",
    "                S_A_count[(S, A)] += 1\n",
    "                Q[S][A] = returns[(S, A)] / S_A_count[(S, A)]\n",
    "\n",
    "            distinct_states = set([s for s, a in S_A])\n",
    "\n",
    "            for s in distinct_states:\n",
    "                a_star = np.argmax(Q[s])\n",
    "                for a in range(self.Num_A):\n",
    "                    if a == a_star:\n",
    "                        Pi[s][a] = 1 - self.epsilon + self.epsilon / self.Num_A\n",
    "                    else:\n",
    "                        Pi[s][a] = self.epsilon / self.Num_A\n",
    "\n",
    "        return Pi\n",
    "\n",
    "    def off_policy_monte_carlo(self, num_iter=10):\n",
    "\n",
    "        self.epsilon = 0.3\n",
    "\n",
    "        NUM_ITER = num_iter\n",
    "\n",
    "        Q = collections.defaultdict(lambda: np.ones(self.Num_A)/self.Num_A)\n",
    "        C = collections.defaultdict(lambda: np.zeros(self.Num_A))\n",
    "        \n",
    "        Mu = self.soft_policy()\n",
    "\n",
    "        for it in range(NUM_ITER):\n",
    "            episode = self.generate_episode(Mu)\n",
    "            if it % 50 == 0:\n",
    "                print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "            G = 0\n",
    "            W = 1\n",
    "            \n",
    "            state = set()\n",
    "            for it_, info in enumerate(episode[::-1]):\n",
    "                S, A, R = info\n",
    "                \n",
    "                state.add(S)\n",
    "                \n",
    "                G = self.gamma * G + R\n",
    "                \n",
    "                C[S][A] += W\n",
    "                Q[S][A] = Q[S][A] + (W/C[S][A]) * (G - Q[S][A])\n",
    "\n",
    "                Pi = self.epsilon_greedy_policy(Q, S)\n",
    "                \n",
    "                W = W * Pi / Mu[S, A]\n",
    "                \n",
    "                if W == 0:\n",
    "                    print(S, A, R, it_)\n",
    "                    break\n",
    "\n",
    "            print(state)\n",
    "        pprint(Q)\n",
    "        return Q\n",
    "\n",
    "    def q_learning(self, num_iter=301):\n",
    "\n",
    "        self.epsilon = .9\n",
    "\n",
    "        NUM_ITER = num_iter\n",
    "\n",
    "        Q = collections.defaultdict(lambda: np.zeros(self.Num_A))\n",
    "\n",
    "        for it in range(NUM_ITER):\n",
    "\n",
    "            if it % 50 == 0:\n",
    "                print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "            S = self.env.reset()\n",
    "\n",
    "            # self.epsilon = max(self.epsilon * 0.999, 0.1)\n",
    "\n",
    "            while True:\n",
    "                A = self.epsilon_greedy_policy(Q, S)\n",
    "                S_, R, terminal, _ = self.env.step(A)\n",
    "\n",
    "                Q[S][A] += self.alpha * (R + self.gamma * max(Q[S_]) - Q[S][A])\n",
    "\n",
    "                S = S_\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "\n",
    "        return Q\n",
    "\n",
    "    def sarsa(self, num_iter=301):\n",
    "\n",
    "        self.epsilon = .9\n",
    "\n",
    "        NUM_ITER = num_iter\n",
    "\n",
    "        Q = collections.defaultdict(lambda: np.zeros(self.Num_A))\n",
    "\n",
    "        for it in range(NUM_ITER):\n",
    "            if it % 50 == 0:\n",
    "                print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "            S = self.env.reset()\n",
    "            A = self.epsilon_greedy_policy(Q, S)\n",
    "\n",
    "            #self.epsilon = max(self.epsilon * 0.999, 0.1)\n",
    "\n",
    "            while True:\n",
    "                S_, R, terminal, _ = self.env.step(A)\n",
    "                A_ = self.epsilon_greedy_policy(Q, S_)\n",
    "                \n",
    "                Q[S][A] += self.alpha*(R + (self.gamma * Q[S_][A_]) - Q[S][A])\n",
    "\n",
    "                S = S_\n",
    "                A = A_\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "        return Q\n",
    "\n",
    "    def generate_episode(self, Pi):\n",
    "\n",
    "        episode = []\n",
    "\n",
    "        S = self.env.reset()\n",
    "        while True:\n",
    "            A = np.random.choice(np.arange(self.Num_A), p=Pi[S])\n",
    "            S_, R, terminal, _ = self.env.step(A)\n",
    "            episode.append((S, A, R))\n",
    "            S = S_\n",
    "            if terminal:\n",
    "                break\n",
    "        return episode\n",
    "\n",
    "    def show_policy(self, Q):\n",
    "\n",
    "        MAX_STEPS = 500\n",
    "\n",
    "        S = self.env.reset()\n",
    "        print(f\"Starting state: {S}\")\n",
    "        # self.env.render()\n",
    "\n",
    "        step = 0\n",
    "        while step < MAX_STEPS:\n",
    "            # A = self.epsilon_greedy_policy(Q, S)\n",
    "            A = np.argmax(Q[S])\n",
    "            S_, R, done, _ = self.env.step(A)\n",
    "            # self.env.render()\n",
    "            if done:\n",
    "                break\n",
    "            print(\n",
    "                f\"Current State: {S}, action: {A}, reward: {R}, done: {done}, step: {step}\"\n",
    "            )\n",
    "            S = S_\n",
    "            step += 1\n",
    "        print(\n",
    "            f\"Current State: {S}, action: {A}, reward: {R}, done: {done}, step: {step}\"\n",
    "        )\n",
    "        # self.env.render()\n",
    "        self.env.close()\n",
    "        print(\"Finished\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = Agent(environment='CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 24, -1, False)],\n",
       " 1: [(1.0, 36, -100, False)],\n",
       " 2: [(1.0, 36, -1, False)],\n",
       " 3: [(1.0, 36, -1, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.env.P[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "S = agent.env.reset()\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Policy:,  [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "Generating Episode Number: 0\n"
     ]
    }
   ],
   "source": [
    "on_policy = agent.on_policy_monte_carlo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state: 36\n",
      "Current State: 36, action: 0, reward: -1, done: False, step: 0\n",
      "Current State: 24, action: 0, reward: -1, done: False, step: 1\n",
      "Current State: 12, action: 0, reward: -1, done: False, step: 2\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 3\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 4\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 5\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 6\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 7\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 8\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 9\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 10\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 11\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 12\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 13\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 14\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 15\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 16\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 17\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 18\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 19\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 20\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 21\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 22\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 23\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 24\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 25\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 26\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 27\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 28\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 29\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 30\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 31\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 32\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 33\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 34\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 35\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 36\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 37\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 38\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 39\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 40\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 41\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 42\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 43\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 44\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 45\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 46\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 47\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 48\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 49\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 50\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 51\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 52\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 53\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 54\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 55\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 56\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 57\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 58\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 59\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 60\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 61\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 62\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 63\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 64\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 65\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 66\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 67\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 68\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 69\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 70\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 71\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 72\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 73\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 74\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 75\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 76\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 77\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 78\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 79\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 80\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 81\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 82\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 83\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 84\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 85\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 86\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 87\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 88\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 89\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 90\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 91\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 92\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 93\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 94\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 95\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 96\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 97\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 98\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 99\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 100\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 101\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 102\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 103\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 104\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 105\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 106\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 107\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 108\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 109\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 110\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 111\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 112\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 113\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 114\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 115\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 116\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 117\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 118\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 119\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 120\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 121\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 122\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 123\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 124\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 125\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 126\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 127\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 128\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 129\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 130\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 131\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 132\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 133\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 134\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 135\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 136\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 137\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 138\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 139\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 140\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 141\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 142\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 143\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 144\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 145\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 146\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 147\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 148\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 149\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 150\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 151\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 152\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 153\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 154\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 155\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 156\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 157\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 158\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 159\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 160\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 161\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 162\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 163\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 164\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 165\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 166\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 167\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 168\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 169\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 170\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 171\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 172\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 173\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 174\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 175\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 176\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 177\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 178\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 179\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 180\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 181\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 182\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 183\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 184\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 185\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 186\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 187\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 188\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 189\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 190\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 191\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 192\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 193\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 194\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 195\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 196\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 197\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 198\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 199\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 200\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 201\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 202\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 203\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 204\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 205\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 206\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 207\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 208\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 209\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 210\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 211\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 212\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 213\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 214\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 215\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 216\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 217\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 218\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 219\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 220\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 221\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 222\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 223\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 224\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 225\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 226\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 227\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 228\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 229\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 230\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 231\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 232\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 233\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 234\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 235\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 236\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 237\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 238\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 239\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 240\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 241\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 242\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 243\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 244\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 245\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 246\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 247\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 248\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 249\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 250\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 251\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 252\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 253\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 254\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 255\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 256\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 257\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 258\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 259\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 260\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 261\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 262\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 263\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 264\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 265\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 266\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 267\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 268\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 269\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 270\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 271\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 272\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 273\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 274\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 275\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 276\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 277\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 278\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 279\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 280\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 281\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 282\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 283\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 284\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 285\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 286\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 287\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 288\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 289\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 290\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 291\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 292\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 293\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 294\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 295\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 296\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 297\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 298\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 299\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 300\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 301\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 302\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 303\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 304\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 305\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 306\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 307\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 308\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 309\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 310\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 311\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 312\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 313\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 314\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 315\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 316\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 317\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 318\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 319\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 320\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 321\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 322\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 323\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 324\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 325\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 326\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 327\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 328\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 329\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 330\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 331\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 332\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 333\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 334\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 335\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 336\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 337\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 338\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 339\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 340\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 341\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 342\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 343\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 344\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 345\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 346\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 347\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 348\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 349\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 350\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 351\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 352\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 353\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 354\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 355\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 356\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 357\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 358\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 359\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 360\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 361\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 362\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 363\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 364\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 365\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 366\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 367\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 368\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 369\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 370\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 371\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 372\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 373\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 374\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 375\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 376\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 377\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 378\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 379\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 380\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 381\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 382\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 383\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 384\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 385\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 386\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 387\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 388\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 389\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 390\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 391\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 392\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 393\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 394\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 395\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 396\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 397\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 398\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 399\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 400\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 401\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 402\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 403\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 404\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 405\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 406\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 407\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 408\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 409\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 410\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 411\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 412\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 413\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 414\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 415\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 416\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 417\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 418\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 419\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 420\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 421\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 422\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 423\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 424\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 425\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 426\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 427\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 428\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 429\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 430\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 431\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 432\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 433\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 434\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 435\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 436\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 437\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 438\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 439\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 440\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 441\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 442\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 443\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 444\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 445\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 446\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 447\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 448\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 449\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 450\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 451\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 452\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 453\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 454\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 455\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 456\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 457\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 458\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 459\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 460\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 461\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 462\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 463\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 464\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 465\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 466\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 467\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 468\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 469\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 470\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 471\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 472\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 473\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 474\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 475\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 476\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 477\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 478\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 479\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 480\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 481\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 482\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 483\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 484\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 485\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 486\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 487\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 488\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 489\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 490\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 491\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 492\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 493\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 494\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 495\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 496\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 497\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 498\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 499\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 500\n",
      "Finished False\n"
     ]
    }
   ],
   "source": [
    "agent.show_policy(on_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Episode Number: 0\n",
      "35 2 0 0\n",
      "{35}\n",
      "35 2 0 0\n",
      "{35}\n",
      "22 2 -1 2\n",
      "{34, 35, 22}\n",
      "35 2 0 0\n",
      "{35}\n",
      "35 2 0 0\n",
      "{35}\n",
      "35 2 0 0\n",
      "{35}\n",
      "35 2 0 0\n",
      "{35}\n",
      "23 2 -1 1\n",
      "{35, 23}\n",
      "35 2 0 0\n",
      "{35}\n",
      "35 2 0 0\n",
      "{35}\n",
      "defaultdict(<function Agent.off_policy_monte_carlo.<locals>.<lambda> at 0x7f80aad33d30>,\n",
      "            {22: array([ 0.25,  0.25, -1.1 ,  0.25]),\n",
      "             23: array([ 0.25,  0.25, -1.  ,  0.25]),\n",
      "             34: array([ 0.25, -1.  ,  0.25,  0.25]),\n",
      "             35: array([0.25, 0.25, 0.  , 0.25])})\n"
     ]
    }
   ],
   "source": [
    "off_policy = agent.off_policy_monte_carlo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state: 36\n",
      "Current State: 36, action: 0, reward: -1, done: False, step: 0\n",
      "Current State: 24, action: 0, reward: -1, done: False, step: 1\n",
      "Current State: 12, action: 0, reward: -1, done: False, step: 2\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 3\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 4\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 5\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 6\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 7\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 8\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 9\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 10\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 11\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 12\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 13\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 14\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 15\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 16\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 17\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 18\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 19\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 20\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 21\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 22\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 23\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 24\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 25\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 26\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 27\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 28\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 29\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 30\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 31\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 32\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 33\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 34\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 35\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 36\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 37\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 38\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 39\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 40\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 41\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 42\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 43\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 44\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 45\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 46\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 47\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 48\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 49\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 50\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 51\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 52\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 53\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 54\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 55\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 56\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 57\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 58\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 59\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 60\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 61\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 62\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 63\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 64\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 65\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 66\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 67\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 68\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 69\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 70\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 71\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 72\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 73\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 74\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 75\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 76\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 77\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 78\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 79\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 80\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 81\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 82\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 83\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 84\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 85\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 86\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 87\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 88\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 89\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 90\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 91\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 92\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 93\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 94\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 95\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 96\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 97\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 98\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 99\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 100\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 101\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 102\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 103\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 104\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 105\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 106\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 107\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 108\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 109\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 110\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 111\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 112\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 113\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 114\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 115\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 116\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 117\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 118\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 119\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 120\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 121\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 122\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 123\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 124\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 125\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 126\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 127\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 128\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 129\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 130\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 131\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 132\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 133\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 134\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 135\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 136\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 137\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 138\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 139\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 140\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 141\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 142\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 143\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 144\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 145\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 146\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 147\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 148\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 149\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 150\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 151\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 152\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 153\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 154\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 155\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 156\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 157\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 158\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 159\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 160\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 161\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 162\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 163\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 164\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 165\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 166\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 167\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 168\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 169\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 170\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 171\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 172\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 173\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 174\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 175\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 176\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 177\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 178\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 179\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 180\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 181\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 182\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 183\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 184\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 185\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 186\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 187\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 188\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 189\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 190\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 191\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 192\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 193\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 194\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 195\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 196\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 197\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 198\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 199\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 200\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 201\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 202\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 203\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 204\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 205\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 206\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 207\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 208\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 209\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 210\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 211\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 212\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 213\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 214\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 215\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 216\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 217\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 218\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 219\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 220\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 221\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 222\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 223\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 224\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 225\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 226\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 227\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 228\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 229\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 230\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 231\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 232\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 233\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 234\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 235\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 236\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 237\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 238\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 239\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 240\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 241\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 242\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 243\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 244\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 245\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 246\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 247\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 248\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 249\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 250\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 251\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 252\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 253\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 254\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 255\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 256\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 257\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 258\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 259\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 260\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 261\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 262\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 263\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 264\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 265\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 266\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 267\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 268\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 269\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 270\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 271\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 272\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 273\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 274\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 275\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 276\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 277\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 278\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 279\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 280\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 281\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 282\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 283\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 284\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 285\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 286\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 287\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 288\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 289\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 290\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 291\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 292\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 293\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 294\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 295\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 296\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 297\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 298\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 299\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 300\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 301\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 302\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 303\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 304\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 305\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 306\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 307\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 308\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 309\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 310\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 311\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 312\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 313\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 314\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 315\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 316\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 317\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 318\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 319\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 320\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 321\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 322\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 323\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 324\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 325\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 326\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 327\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 328\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 329\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 330\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 331\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 332\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 333\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 334\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 335\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 336\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 337\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 338\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 339\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 340\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 341\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 342\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 343\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 344\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 345\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 346\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 347\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 348\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 349\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 350\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 351\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 352\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 353\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 354\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 355\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 356\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 357\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 358\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 359\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 360\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 361\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 362\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 363\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 364\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 365\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 366\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 367\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 368\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 369\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 370\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 371\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 372\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 373\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 374\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 375\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 376\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 377\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 378\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 379\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 380\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 381\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 382\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 383\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 384\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 385\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 386\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 387\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 388\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 389\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 390\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 391\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 392\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 393\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 394\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 395\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 396\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 397\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 398\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 399\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 400\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 401\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 402\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 403\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 404\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 405\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 406\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 407\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 408\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 409\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 410\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 411\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 412\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 413\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 414\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 415\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 416\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 417\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 418\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 419\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 420\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 421\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 422\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 423\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 424\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 425\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 426\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 427\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 428\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 429\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 430\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 431\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 432\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 433\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 434\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 435\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 436\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 437\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 438\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 439\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 440\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 441\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 442\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 443\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 444\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 445\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 446\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 447\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 448\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 449\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 450\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 451\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 452\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 453\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 454\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 455\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 456\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 457\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 458\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 459\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 460\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 461\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 462\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 463\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 464\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 465\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 466\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 467\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 468\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 469\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 470\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 471\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 472\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 473\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 474\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 475\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 476\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 477\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 478\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 479\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 480\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 481\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 482\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 483\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 484\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 485\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 486\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 487\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 488\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 489\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 490\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 491\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 492\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 493\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 494\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 495\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 496\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 497\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 498\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 499\n",
      "Current State: 0, action: 0, reward: -1, done: False, step: 500\n",
      "Finished False\n"
     ]
    }
   ],
   "source": [
    "agent.show_policy(off_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Episode Number: 0\n",
      "Generating Episode Number: 50\n",
      "Generating Episode Number: 100\n",
      "Generating Episode Number: 150\n",
      "Generating Episode Number: 200\n",
      "Generating Episode Number: 250\n",
      "Generating Episode Number: 300\n"
     ]
    }
   ],
   "source": [
    "q_policy = agent.q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state: 36\n",
      "Current State: 36, action: 0, reward: -1, done: False, step: 0\n",
      "Current State: 24, action: 1, reward: -1, done: False, step: 1\n",
      "Current State: 25, action: 1, reward: -1, done: False, step: 2\n",
      "Current State: 26, action: 1, reward: -1, done: False, step: 3\n",
      "Current State: 27, action: 1, reward: -1, done: False, step: 4\n",
      "Current State: 28, action: 1, reward: -1, done: False, step: 5\n",
      "Current State: 29, action: 1, reward: -1, done: False, step: 6\n",
      "Current State: 30, action: 1, reward: -1, done: False, step: 7\n",
      "Current State: 31, action: 1, reward: -1, done: False, step: 8\n",
      "Current State: 32, action: 1, reward: -1, done: False, step: 9\n",
      "Current State: 33, action: 1, reward: -1, done: False, step: 10\n",
      "Current State: 34, action: 1, reward: -1, done: False, step: 11\n",
      "Current State: 35, action: 2, reward: 0, done: True, step: 12\n",
      "Finished True\n"
     ]
    }
   ],
   "source": [
    "agent.show_policy(q_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Episode Number: 0\n",
      "Generating Episode Number: 50\n",
      "Generating Episode Number: 100\n",
      "Generating Episode Number: 150\n",
      "Generating Episode Number: 200\n",
      "Generating Episode Number: 250\n",
      "Generating Episode Number: 300\n"
     ]
    }
   ],
   "source": [
    "sarsa_policy = agent.sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state: 36\n",
      "Current State: 36, action: 0, reward: -1, done: False, step: 0\n",
      "Current State: 24, action: 0, reward: -1, done: False, step: 1\n",
      "Current State: 12, action: 0, reward: -1, done: False, step: 2\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 3\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 4\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 5\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 6\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 7\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 8\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 9\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 10\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 11\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 12\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 13\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 14\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 15\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 16\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 17\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 18\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 19\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 20\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 21\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 22\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 23\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 24\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 25\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 26\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 27\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 28\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 29\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 30\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 31\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 32\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 33\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 34\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 35\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 36\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 37\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 38\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 39\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 40\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 41\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 42\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 43\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 44\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 45\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 46\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 47\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 48\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 49\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 50\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 51\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 52\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 53\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 54\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 55\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 56\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 57\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 58\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 59\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 60\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 61\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 62\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 63\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 64\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 65\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 66\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 67\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 68\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 69\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 70\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 71\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 72\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 73\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 74\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 75\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 76\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 77\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 78\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 79\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 80\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 81\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 82\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 83\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 84\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 85\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 86\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 87\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 88\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 89\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 90\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 91\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 92\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 93\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 94\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 95\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 96\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 97\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 98\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 99\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 100\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 101\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 102\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 103\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 104\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 105\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 106\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 107\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 108\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 109\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 110\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 111\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 112\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 113\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 114\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 115\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 116\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 117\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 118\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 119\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 120\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 121\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 122\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 123\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 124\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 125\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 126\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 127\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 128\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 129\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 130\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 131\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 132\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 133\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 134\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 135\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 136\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 137\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 138\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 139\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 140\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 141\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 142\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 143\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 144\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 145\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 146\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 147\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 148\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 149\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 150\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 151\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 152\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 153\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 154\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 155\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 156\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 157\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 158\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 159\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 160\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 161\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 162\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 163\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 164\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 165\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 166\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 167\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 168\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 169\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 170\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 171\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 172\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 173\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 174\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 175\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 176\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 177\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 178\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 179\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 180\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 181\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 182\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 183\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 184\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 185\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 186\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 187\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 188\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 189\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 190\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 191\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 192\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 193\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 194\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 195\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 196\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 197\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 198\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 199\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 200\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 201\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 202\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 203\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 204\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 205\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 206\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 207\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 208\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 209\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 210\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 211\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 212\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 213\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 214\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 215\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 216\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 217\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 218\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 219\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 220\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 221\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 222\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 223\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 224\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 225\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 226\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 227\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 228\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 229\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 230\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 231\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 232\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 233\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 234\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 235\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 236\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 237\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 238\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 239\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 240\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 241\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 242\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 243\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 244\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 245\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 246\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 247\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 248\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 249\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 250\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 251\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 252\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 253\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 254\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 255\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 256\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 257\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 258\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 259\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 260\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 261\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 262\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 263\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 264\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 265\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 266\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 267\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 268\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 269\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 270\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 271\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 272\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 273\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 274\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 275\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 276\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 277\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 278\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 279\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 280\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 281\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 282\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 283\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 284\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 285\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 286\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 287\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 288\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 289\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 290\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 291\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 292\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 293\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 294\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 295\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 296\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 297\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 298\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 299\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 300\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 301\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 302\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 303\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 304\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 305\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 306\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 307\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 308\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 309\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 310\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 311\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 312\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 313\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 314\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 315\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 316\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 317\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 318\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 319\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 320\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 321\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 322\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 323\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 324\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 325\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 326\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 327\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 328\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 329\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 330\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 331\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 332\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 333\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 334\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 335\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 336\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 337\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 338\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 339\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 340\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 341\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 342\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 343\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 344\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 345\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 346\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 347\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 348\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 349\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 350\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 351\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 352\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 353\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 354\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 355\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 356\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 357\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 358\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 359\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 360\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 361\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 362\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State: 0, action: 3, reward: -1, done: False, step: 364\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 365\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 366\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 367\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 368\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 369\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 370\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 371\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 372\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 373\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 374\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 375\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 376\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 377\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 378\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 379\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 380\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 381\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 382\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 383\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 384\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 385\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 386\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 387\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 388\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 389\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 390\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 391\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 392\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 393\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 394\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 395\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 396\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 397\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 398\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 399\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 400\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 401\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 402\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 403\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 404\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 405\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 406\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 407\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 408\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 409\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 410\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 411\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 412\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 413\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 414\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 415\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 416\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 417\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 418\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 419\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 420\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 421\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 422\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 423\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 424\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 425\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 426\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 427\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 428\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 429\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 430\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 431\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 432\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 433\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 434\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 435\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 436\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 437\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 438\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 439\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 440\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 441\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 442\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 443\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 444\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 445\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 446\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 447\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 448\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 449\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 450\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 451\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 452\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 453\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 454\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 455\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 456\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 457\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 458\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 459\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 460\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 461\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 462\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 463\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 464\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 465\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 466\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 467\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 468\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 469\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 470\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 471\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 472\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 473\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 474\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 475\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 476\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 477\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 478\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 479\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 480\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 481\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 482\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 483\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 484\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 485\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 486\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 487\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 488\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 489\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 490\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 491\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 492\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 493\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 494\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 495\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 496\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 497\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 498\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 499\n",
      "Current State: 0, action: 3, reward: -1, done: False, step: 500\n",
      "Finished False\n"
     ]
    }
   ],
   "source": [
    "agent.show_policy(sarsa_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{36: array([  -1.20091309, -101.74962122,   -3.65018643,   -2.48144622]),\n",
       " 24: array([-1.1146366 , -3.52126048, -2.29739163, -1.26445332]),\n",
       " 25: array([  -1.14796998,   -4.41455618, -103.83050652,   -1.22181707]),\n",
       " 26: array([  -1.14452302,   -1.41895577, -102.61450119,   -2.78873296]),\n",
       " 13: array([-1.11161537, -1.18458809, -1.72359029, -1.11656789]),\n",
       " 12: array([-1.11123377, -1.1366191 , -1.2648748 , -1.11401786]),\n",
       " 0: array([-1.11126455, -1.11149955, -1.11619181, -1.11121115]),\n",
       " 1: array([-1.11200075, -1.11162754, -1.12773244, -1.11123554]),\n",
       " 2: array([-1.11228749, -1.1129993 , -1.17113046, -1.11180111]),\n",
       " 14: array([-1.11258206, -1.1225903 , -5.15097387, -1.15964637]),\n",
       " 3: array([-1.11229811, -1.11329454, -1.18232036, -1.11192564]),\n",
       " 4: array([-1.11331542, -1.11239441, -1.1885532 , -1.11238761]),\n",
       " 16: array([-1.11490012, -1.14141725, -3.85584926, -1.13550054]),\n",
       " 28: array([  -1.13963841,   -3.99490059, -103.78844717,   -5.64267201]),\n",
       " 29: array([  -1.20178896,   -2.72291147, -103.35401472,   -2.80334742]),\n",
       " 17: array([-1.11300054, -1.15409559, -3.4475559 , -1.12440595]),\n",
       " 5: array([-1.11289941, -1.11259564, -1.15003351, -1.11198743]),\n",
       " 15: array([-1.11288464, -1.20813198, -4.24487577, -1.18173909]),\n",
       " 27: array([  -1.17677673,   -2.73115663, -101.89660016,   -4.04387579]),\n",
       " 18: array([-1.1135266 , -1.1819193 , -4.71761513, -1.14574514]),\n",
       " 19: array([-1.1132001 , -1.1346056 , -5.0351283 , -1.16471058]),\n",
       " 7: array([-1.11525786, -1.11214725, -1.21607078, -1.11198372]),\n",
       " 8: array([-1.11434979, -1.11194965, -1.20067634, -1.11148779]),\n",
       " 9: array([-1.11216205, -1.11290065, -1.14077381, -1.11323805]),\n",
       " 10: array([-1.11197967, -1.11120638, -1.1473712 , -1.1126859 ]),\n",
       " 22: array([-1.11228088, -1.11269142, -3.78997056, -1.16647856]),\n",
       " 11: array([-1.11120102, -1.11119218, -1.11315522, -1.11254674]),\n",
       " 23: array([-1.11121648, -1.11339282, -1.14299095, -1.17658024]),\n",
       " 35: array([-1.11327011, -1.13703189,  0.        , -4.74514617]),\n",
       " 34: array([  -1.19737373,   -1.17823181, -103.07693625,   -4.66153429]),\n",
       " 21: array([-1.11261337, -1.17992158, -4.20114855, -1.14139608]),\n",
       " 20: array([-1.11254976, -1.20405407, -4.28162448, -1.18939626]),\n",
       " 32: array([  -1.22487886,   -4.74489266, -101.15713054,   -3.80873269]),\n",
       " 31: array([  -1.21225278,   -3.48010333, -103.51558373,   -3.4715594 ]),\n",
       " 30: array([  -1.15664359,   -3.37464782, -101.63050597,   -4.18082025]),\n",
       " 6: array([-1.11168512, -1.11297869, -1.24566235, -1.11389922]),\n",
       " 33: array([  -1.14816486,   -2.75121701, -103.06765807,   -3.44520887]),\n",
       " 47: array([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(sarsa_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{36: array([  -1.11111111, -100.11111111,   -1.11111111,   -1.11111111]),\n",
       " 24: array([-1.11111111, -1.11111111, -1.11111111, -1.11111111]),\n",
       " 25: array([  -1.11111111,   -1.11111111, -100.11111111,   -1.11111111]),\n",
       " 26: array([  -1.11111111,   -1.11111111, -100.11111111,   -1.11111111]),\n",
       " 14: array([-1.11111111, -1.11111111, -1.11111111, -1.11111111]),\n",
       " 13: array([-1.11111111, -1.11111111, -1.11111111, -1.11111111]),\n",
       " 12: array([-1.11111111, -1.11111111, -1.11111111, -1.11111111]),\n",
       " 0: array([-1.11111111, -1.11111111, -1.11111111, -1.11111111]),\n",
       " 1: array([-1.11111111, -1.11111111, -1.11111111, -1.11111111]),\n",
       " 2: array([-1.11111111, -1.11111111, -1.11111111, -1.11111111]),\n",
       " 3: array([-1.11111111, -1.11111111, -1.11111111, -1.11111111]),\n",
       " 15: array([-1.11111111, -1.11111111, -1.11111111, -1.11111111]),\n",
       " 27: array([  -1.11111111,   -1.1111111 , -100.11111111,   -1.11111111]),\n",
       " 28: array([  -1.11111111,   -1.111111  , -100.11111111,   -1.11111111]),\n",
       " 16: array([-1.11111111, -1.1111111 , -1.1111111 , -1.11111111]),\n",
       " 29: array([  -1.1111111 ,   -1.11111   , -100.11111111,   -1.1111111 ]),\n",
       " 4: array([-1.11111111, -1.11111111, -1.11111111, -1.11111111]),\n",
       " 5: array([-1.11111111, -1.1111111 , -1.1111111 , -1.11111111]),\n",
       " 17: array([-1.11111111, -1.111111  , -1.111111  , -1.11111111]),\n",
       " 30: array([  -1.111111  ,   -1.1111    , -100.11111111,   -1.111111  ]),\n",
       " 18: array([-1.1111111, -1.11111  , -1.11111  , -1.1111111]),\n",
       " 6: array([-1.1111111 , -1.111111  , -1.111111  , -1.11111111]),\n",
       " 7: array([-1.111111 , -1.11111  , -1.11111  , -1.1111111]),\n",
       " 19: array([-1.111111, -1.1111  , -1.1111  , -1.111111]),\n",
       " 20: array([-1.11111, -1.111  , -1.111  , -1.11111]),\n",
       " 32: array([  -1.1111    ,   -1.11      , -100.11111111,   -1.1111    ]),\n",
       " 33: array([  -1.111     ,   -1.1       , -100.11111111,   -1.111     ]),\n",
       " 21: array([-1.1111, -1.11  , -1.11  , -1.1111]),\n",
       " 34: array([  -1.11      ,   -1.        , -100.11111107,   -1.11      ]),\n",
       " 22: array([-1.111, -1.1  , -1.1  , -1.111]),\n",
       " 23: array([-1.11, -1.1 , -1.  , -1.11]),\n",
       " 11: array([-1.11 , -1.11 , -1.1  , -1.111]),\n",
       " 10: array([-1.111 , -1.11  , -1.11  , -1.1111]),\n",
       " 35: array([-1.1, -1. ,  0. , -1.1]),\n",
       " 31: array([  -1.11111   ,   -1.111     , -100.11111111,   -1.11111   ]),\n",
       " 8: array([-1.11111 , -1.1111  , -1.1111  , -1.111111]),\n",
       " 9: array([-1.1111 , -1.111  , -1.111  , -1.11111]),\n",
       " 47: array([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(q_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sarsa_policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob = np.array([np.exp(x) for x in sarsa_policy[0]])\n",
    "# prob = prob/(sum(prob))\n",
    "# print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the environment\n",
    "env = gym.make('Roulette-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_policy(nA):\n",
    "    \"\"\"\n",
    "    Creates a random policy function.\n",
    "    \n",
    "    Args:\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes an observation as input and returns a vector\n",
    "        of action probabilities\n",
    "    \"\"\"\n",
    "    A = np.ones(nA, dtype=float) / nA\n",
    "    def policy_fn(observation):\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_policy(Q):\n",
    "    \"\"\"\n",
    "    Creates a greedy policy based on Q values.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action values\n",
    "        \n",
    "    Returns:\n",
    "        A function that takes an observation as input and returns a vector\n",
    "        of action probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        A = np.zeros_like(Q[state], dtype=float)\n",
    "        best_action = np.argmax(Q[state])\n",
    "        A[best_action] = 1.0\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_importance_sampling(env, num_episodes, behavior_policy, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control Off-Policy Control using Weighted Importance Sampling.\n",
    "    Finds an optimal greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        behavior_policy: The behavior to follow while generating episodes.\n",
    "            A function that given an observation returns a vector of probabilities for each action.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities. This is the optimal greedy policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A dictionary that maps state -> action values\n",
    "    Q = collections.defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    # The cumulative denominator of the weighted importance sampling formula\n",
    "    # (across all episodes)\n",
    "    C = collections.defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # Our greedily policy we want to learn\n",
    "    target_policy = create_greedy_policy(Q)\n",
    "        \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # Generate an episode.\n",
    "        # An episode is an array of (state, action, reward) tuples\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        for t in range(100):\n",
    "            # Sample an action from our policy\n",
    "            probs = behavior_policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        # Sum of discounted returns\n",
    "        G = 0.0\n",
    "        # The importance sampling ratio (the weights of the returns)\n",
    "        W = 1.0\n",
    "        # For each step in the episode, backwards\n",
    "        for t in range(len(episode))[::-1]:\n",
    "            state, action, reward = episode[t]\n",
    "            # Update the total reward since step t\n",
    "            G = discount_factor * G + reward\n",
    "            # Update weighted importance sampling formula denominator\n",
    "            C[state][action] += W\n",
    "            # Update the action-value function using the incremental update formula (5.7)\n",
    "            # This also improves our target policy which holds a reference to Q\n",
    "            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
    "            # If the action taken by the behavior policy is not the action \n",
    "            # taken by the target policy the probability will be 0 and we can break\n",
    "            if action !=  np.argmax(target_policy(state)):\n",
    "                break\n",
    "            W = W * 1./behavior_policy(state)[action]\n",
    "        \n",
    "    return Q, target_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15000/15000."
     ]
    }
   ],
   "source": [
    "random_policy = create_random_policy(env.action_space.n)\n",
    "Q, policy = mc_control_importance_sampling(env, num_episodes=15000, behavior_policy=random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.mc_control_importance_sampling.<locals>.<lambda>()>,\n",
       "            {0: array([-2.93646592,  0.04      ,  0.03448276,  0.125     , -0.90627063,\n",
       "                    -0.18032787, -1.26666667, -0.03125   ,  0.18518519, -0.1       ,\n",
       "                     3.98123626, -0.98828125,  0.0625    , -0.30434783, -0.23529412,\n",
       "                    -0.22580645,  0.15151515, -0.08571429, -0.02702703, -0.15151515,\n",
       "                     1.32786885,  0.01886792,  2.99996739, -1.21212121, -0.2173913 ,\n",
       "                    -0.03448276, -0.875     ,  2.86129458, -1.10958904, -0.14285714,\n",
       "                     1.33928571, -0.07142857, -0.125     , -0.21428571, -0.90633245,\n",
       "                     1.11940299, -1.25      ,  0.        ])})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state: 36\n",
      "Current State: 36, action: 0, reward: -1, done: False, step: 0\n",
      "Current State: 24, action: 0, reward: -1, done: False, step: 1\n",
      "Current State: 12, action: 0, reward: -1, done: False, step: 2\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w_/qlbstr_52f97pcd9cbp9m_l00000gn/T/ipykernel_33969/1207441410.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/w_/qlbstr_52f97pcd9cbp9m_l00000gn/T/ipykernel_33969/794741015.py\u001b[0m in \u001b[0;36mshow_policy\u001b[0;34m(self, Q)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;31m# A = self.epsilon_greedy_policy(Q, S)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mS_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0;31m# self.env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/gym/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/gym/lib/python3.9/site-packages/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "agent.show_policy(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
