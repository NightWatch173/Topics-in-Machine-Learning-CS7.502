{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/kanishkjain/opt/anaconda3/envs/gym/lib/python3.9/site-packages\")\n",
    "\n",
    "import random\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "import gym\n",
    "import gym_toytext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, environment=\"Roulette-v0\"\n",
    "    ) -> None:\n",
    "\n",
    "        self.environment = environment\n",
    "\n",
    "    def epsilon_greedy_policy(self, Pi, epsilon):\n",
    "        \n",
    "        Pi = np.around(Pi, decimals=1)\n",
    "\n",
    "        p = np.random.rand()\n",
    "        Num_A = len(Pi)\n",
    "        if p < epsilon:\n",
    "            return np.random.choice(Num_A)\n",
    "        else:\n",
    "            a_star = np.max(Pi)\n",
    "            max_indices = [i for i in range(Num_A) if Pi[i]==a_star]\n",
    "            A = np.random.choice(max_indices)\n",
    "            return A\n",
    "    \n",
    "    def soft_policy(self, Num_A):\n",
    "        A = np.random.choice(Num_A)\n",
    "        return A\n",
    "    \n",
    "    def on_policy_monte_carlo(self, num_iter=30000, alpha=0.5,  gamma=0.6, epsilon = 0.7):\n",
    "\n",
    "        env = gym.make(self.environment)\n",
    "        Num_A = env.action_space.n\n",
    "        Num_S = env.observation_space.n\n",
    "        env.reset()\n",
    "\n",
    "        NUM_ITER = num_iter\n",
    "\n",
    "        Q = np.zeros((Num_S, Num_A))\n",
    "        C = np.zeros((Num_S, Num_A))\n",
    "        rewards = np.zeros(NUM_ITER)\n",
    "\n",
    "        Pi = np.ones((Num_S, Num_A))/Num_A\n",
    "        \n",
    "        returns = collections.defaultdict(list)\n",
    "\n",
    "        for it in range(1, NUM_ITER + 1):\n",
    "            if it % 5000 == 0:\n",
    "                print(f\"Generating Episode Number: {it}\")\n",
    "                \n",
    "            S = env.reset()\n",
    "            \n",
    "            G = 0\n",
    "            step = 0\n",
    "            episode = []\n",
    "            while True:\n",
    "                if np.random.rand() < epsilon:\n",
    "                    A = np.random.choice(Num_A)\n",
    "                else:\n",
    "                    a_star = np.max(Pi[S])\n",
    "                    max_indices = [i for i in range(Num_A) if Pi[S, i]==a_star]\n",
    "                    A = np.random.choice(max_indices)\n",
    "            \n",
    "                S_, R, terminal, _ = env.step(A)\n",
    "                \n",
    "                if terminal and self.environment == 'CliffWalking-v0':\n",
    "                    R = 0\n",
    "                \n",
    "                episode.append((S, A, R))\n",
    "                \n",
    "                G = G + (gamma**step)*R\n",
    "\n",
    "                S = S_\n",
    "                \n",
    "                step+=1\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "                    \n",
    "            rewards[it - 1] = G\n",
    "\n",
    "            S_A = [(S, A) for (S, A, _) in episode]\n",
    "            \n",
    "            G = 0\n",
    "            t = len(episode) - 1\n",
    "            while t >= 0:\n",
    "                S, A, R = episode[t]\n",
    "                G = G + R\n",
    "                if not (S, A) in S_A[:t]:\n",
    "                    returns[(S, A)].append(G)\n",
    "                    Q[S, A] = sum(returns[(S, A)])/len(returns[(S, A)])\n",
    "                    \n",
    "                    q_max = np.max(Q[S])\n",
    "                    max_indices = [i for i in range(Num_A) if Q[S, i]==q_max]\n",
    "                    A_star = np.random.choice(max_indices)\n",
    "                    \n",
    "                    for a in range(Num_A):\n",
    "                        if a == A_star:\n",
    "                            Pi[S, a] = 1 - epsilon + (epsilon/Num_A)\n",
    "                        else:\n",
    "                            Pi[S, a] = (epsilon/Num_A)\n",
    "                t-=1\n",
    "                \n",
    "        return Q, rewards, Pi\n",
    "    \n",
    "    def off_policy_monte_carlo(self, num_iter=50000, alpha=0.05, gamma=0.0, epsilon = 0.1):\n",
    "\n",
    "        env = gym.make(self.environment)\n",
    "        Num_A = env.action_space.n\n",
    "        Num_S = env.observation_space.n\n",
    "        env.reset()\n",
    "\n",
    "        NUM_ITER = num_iter\n",
    "\n",
    "        # Q = np.random.rand(Num_S, Num_A)\n",
    "        Q = np.zeros((Num_S, Num_A))\n",
    "        C = np.zeros((Num_S, Num_A))\n",
    "        rewards = np.zeros(NUM_ITER)\n",
    "        \n",
    "        Pi = np.ones((Num_S, Num_A))/Num_A\n",
    "\n",
    "        for it in range(1, NUM_ITER + 1):\n",
    "            if it % 5000 == 0:\n",
    "                print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "            S = env.reset()\n",
    "            \n",
    "            G = 0\n",
    "            step = 0\n",
    "            episode = []\n",
    "            while True:    \n",
    "                A = np.random.choice(Num_A)\n",
    "                S_, R, terminal, _ = env.step(A)\n",
    "                \n",
    "                if terminal and self.environment == 'CliffWalking-v0':\n",
    "                    R = 0\n",
    "                \n",
    "                episode.append((S, A, R))\n",
    "                \n",
    "                G = G + (gamma**step)*R\n",
    "\n",
    "                S = S_\n",
    "                \n",
    "                step+=1\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "                    \n",
    "            rewards[it - 1] = G\n",
    "            \n",
    "            G = 0\n",
    "            W = 1\n",
    "            t = len(episode) - 1\n",
    "            while t >= 0:\n",
    "                S, A, R = episode[t]\n",
    "                \n",
    "                G = gamma * G + R\n",
    "                \n",
    "                C[S, A] += W\n",
    "                Q[S, A] += (W/C[S, A]) * (G - Q[S, A])\n",
    "                \n",
    "                q_max = np.max(Q[S])\n",
    "                max_indices = [i for i in range(Num_A) if Q[S, i]==q_max]\n",
    "                A_star = np.random.choice(max_indices)\n",
    "                \n",
    "                for a in range(Num_A):\n",
    "                    if a == A_star:\n",
    "                        Pi[S, a] = (1 - epsilon) + (epsilon/Num_A)\n",
    "                    #if a in max_indices:\n",
    "                    #    Pi[S, a] = (1 - epsilon)/(len(max_indices)) + (epsilon/Num_A)\n",
    "                    else:\n",
    "                        Pi[S, a] = (epsilon/Num_A)\n",
    "                \n",
    "                W = W*(Pi[S, A]*Num_A) #/epsilon\n",
    "                \n",
    "                if W == 0:\n",
    "                    break\n",
    "                \n",
    "                t -= 1\n",
    "\n",
    "        return Q, rewards\n",
    "    \n",
    "    def q_learning(self, num_iter=30000, alpha=0.1, gamma=0.9, epsilon = 0.05):\n",
    "\n",
    "        env = gym.make(self.environment)\n",
    "        Num_A = env.action_space.n\n",
    "        Num_S = env.observation_space.n\n",
    "        env.reset()\n",
    "\n",
    "        NUM_ITER = num_iter\n",
    "\n",
    "        Q = np.zeros((Num_S, Num_A))\n",
    "        rewards = np.zeros(NUM_ITER)\n",
    "\n",
    "        for it in range(1, NUM_ITER + 1):\n",
    "\n",
    "            if it % 5000 == 0:\n",
    "                print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "            S = env.reset()\n",
    "            \n",
    "            G = 0\n",
    "            step = 0\n",
    "            while True:\n",
    "                A = self.epsilon_greedy_policy(Q[S], epsilon)\n",
    "                S_, R, terminal, _ = env.step(A)\n",
    "                \n",
    "                G = G + (gamma**step)*R\n",
    "\n",
    "                Q[S][A] += alpha * (R + gamma * np.max(Q[S_]) - Q[S][A])\n",
    "\n",
    "                S = S_\n",
    "                \n",
    "                step+=1\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "                    \n",
    "            rewards[it -1] = G\n",
    "\n",
    "        return Q, rewards\n",
    "\n",
    "    def sarsa(self, num_iter=30000, alpha=0.1, gamma=0.9, epsilon = 0.05):\n",
    "\n",
    "        env = gym.make(self.environment)\n",
    "        Num_A = env.action_space.n\n",
    "        Num_S = env.observation_space.n\n",
    "        env.reset()\n",
    "\n",
    "        NUM_ITER = num_iter\n",
    "\n",
    "        Q = np.zeros((Num_S, Num_A))\n",
    "        rewards = np.zeros(NUM_ITER)\n",
    "\n",
    "        for it in range(1, NUM_ITER + 1):\n",
    "            if it % 5000 == 0:\n",
    "                print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "            S = env.reset()\n",
    "            A = self.epsilon_greedy_policy(Q[S], epsilon)\n",
    "            \n",
    "            G = 0\n",
    "            step = 0\n",
    "            while True:\n",
    "                S_, R, terminal, _ = env.step(A)\n",
    "                A_ = self.epsilon_greedy_policy(Q[S_], epsilon)\n",
    "                \n",
    "                G = G + (gamma**step)*R\n",
    "\n",
    "                Q[S][A] = Q[S][A] + alpha * (R + gamma * Q[S_][A_] - Q[S][A])\n",
    "\n",
    "                S = S_\n",
    "                A = A_\n",
    "                \n",
    "                step += 1\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "            rewards[it -1] = G\n",
    "\n",
    "        return Q, rewards\n",
    "\n",
    "    def show_policy(self, Q):\n",
    "\n",
    "        MAX_STEPS = 51\n",
    "        \n",
    "        env = gym.make(self.environment)\n",
    "        Num_A = env.action_space.n\n",
    "        Num_S = env.observation_space.n\n",
    "\n",
    "        S = env.reset()\n",
    "        print(f\"Starting state: {S}\")\n",
    "\n",
    "        step = 0\n",
    "        done = False\n",
    "        while step < MAX_STEPS:\n",
    "            A = np.argmax(Q[S])\n",
    "            # env.render()\n",
    "            S_, R, done, _ = env.step(A)\n",
    "            if done:\n",
    "                break\n",
    "            print(\n",
    "                f\"Current State: {S}, action: {A}, reward: {R}, terminal: {done}, step: {step}\"\n",
    "            )\n",
    "            S = S_\n",
    "            step += 1\n",
    "        if done:\n",
    "            print(\n",
    "                f\"Current State: {S}, action: {A}, reward: {R}, terminal: {done}, step: {step}\"\n",
    "            )\n",
    "            # env.render()\n",
    "        env.close()\n",
    "        print(\"Finished\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = Agent(environment='Roulette-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Episode Number: 5000\n",
      "Generating Episode Number: 10000\n",
      "Generating Episode Number: 15000\n",
      "Generating Episode Number: 20000\n",
      "Generating Episode Number: 25000\n",
      "Generating Episode Number: 30000\n",
      "Starting state: 0\n",
      "Current State: 0, action: 37, reward: 0, terminal: True, step: 0\n",
      "Finished True\n"
     ]
    }
   ],
   "source": [
    "q_value, rewards, policy = agent.on_policy_monte_carlo()\n",
    "agent.show_policy(q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Episode Number: 5000\n",
      "Generating Episode Number: 10000\n",
      "Generating Episode Number: 15000\n",
      "Generating Episode Number: 20000\n",
      "Generating Episode Number: 25000\n",
      "Generating Episode Number: 30000\n",
      "Generating Episode Number: 35000\n",
      "Generating Episode Number: 40000\n",
      "Generating Episode Number: 45000\n",
      "Generating Episode Number: 50000\n",
      "Starting state: 0\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 0\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 1\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 2\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 3\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 4\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 5\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 6\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 7\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 8\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 9\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 10\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 11\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 12\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 13\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 14\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 15\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 16\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 17\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 18\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 19\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 20\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 21\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 22\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 23\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 24\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 25\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 26\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 27\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 28\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 29\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 30\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 31\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 32\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 33\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 34\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 35\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 36\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 37\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 38\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 39\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 40\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 41\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 42\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 43\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 44\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 45\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 46\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 47\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 48\n",
      "Current State: 0, action: 28, reward: -1.0, terminal: False, step: 49\n",
      "Current State: 0, action: 28, reward: 1.0, terminal: False, step: 50\n",
      "Finished False\n"
     ]
    }
   ],
   "source": [
    "off_policy, rewards = agent.off_policy_monte_carlo()\n",
    "agent.show_policy(off_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3072497 , -0.99996614, -0.90028068,  0.54515249,  0.21347303,\n",
       "        0.34593049, -0.78845437,  0.41948918, -0.38611758, -0.22053396,\n",
       "       -0.3294432 ,  0.08780172, -0.19371107, -0.99994617,  0.0109279 ,\n",
       "       -0.19209121,  0.20451046, -0.81455185, -0.4230577 , -0.90168653,\n",
       "       -0.14390406,  0.15154567, -0.00395848,  0.00953864, -0.27278296,\n",
       "       -0.26963759,  0.9995746 , -0.9960765 ,  0.99978178,  0.99462403,\n",
       "       -0.02002793, -0.88427229,  0.14254751, -0.13860719,  0.00392699,\n",
       "        0.6087428 ,  0.65780813,  0.        ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_policy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Episode Number: 5000\n",
      "Generating Episode Number: 10000\n",
      "Generating Episode Number: 15000\n",
      "Generating Episode Number: 20000\n",
      "Generating Episode Number: 25000\n",
      "Generating Episode Number: 30000\n",
      "Starting state: 0\n",
      "Current State: 0, action: 37, reward: 0, terminal: True, step: 0\n",
      "Finished True\n"
     ]
    }
   ],
   "source": [
    "q_policy, rewards = agent.q_learning()\n",
    "agent.show_policy(q_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Episode Number: 5000\n",
      "Generating Episode Number: 10000\n",
      "Generating Episode Number: 15000\n",
      "Generating Episode Number: 20000\n",
      "Generating Episode Number: 25000\n",
      "Generating Episode Number: 30000\n",
      "Starting state: 0\n",
      "Current State: 0, action: 37, reward: 0, terminal: True, step: 0\n",
      "Finished True\n"
     ]
    }
   ],
   "source": [
    "sarsa_policy, rewards = agent.sarsa()\n",
    "agent.show_policy(sarsa_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
