{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/kanishkjain/opt/anaconda3/envs/gym/lib/python3.9/site-packages\")\n",
    "\n",
    "import random\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "import gym\n",
    "import gym_toytext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Agent:\n",
    "#     def __init__(\n",
    "#         self, environment=\"Roulette-v0\", gamma=0.1, theta=1e-6, epsilon=1.0, alpha=0.1\n",
    "#     ) -> None:\n",
    "\n",
    "#         self.env = gym.make(environment)\n",
    "#         self.env.reset()\n",
    "\n",
    "#         self.gamma = gamma\n",
    "#         self.theta = theta\n",
    "#         self.epsilon = epsilon\n",
    "\n",
    "#         self.alpha = alpha\n",
    "\n",
    "#         self.A_space = self.env.action_space\n",
    "#         self.S_space = self.env.observation_space\n",
    "#         self.R_range = self.env.reward_range\n",
    "\n",
    "#         self.Num_A = self.A_space.n\n",
    "#         self.Num_S = self.S_space.n\n",
    "        \n",
    "#         if environment == 'CliffWalking-v0':\n",
    "#             for s in range(self.Num_S):\n",
    "#                 for a in range(self.Num_A):\n",
    "#                     P, S_, R_, T = self.env.P[s][a][0]\n",
    "#                     if T:\n",
    "#                         self.env.P[s][a] = [(P, S_, 0, T)]\n",
    "\n",
    "#     def soft_policy(self):\n",
    "\n",
    "#         Pi = np.ones((self.Num_S, self.Num_A)) / self.Num_A\n",
    "#         return Pi\n",
    "\n",
    "#     def greedy_policy(self, Q):\n",
    "#         Pi = np.zeros((self.Num_S, self.Num_A))\n",
    "#         for s in range(self.Num_S):\n",
    "#             a_star = np.argmax([Q[(s, a)] for a in range(self.Num_A)])\n",
    "#             Pi[s, a_star] = 1.0\n",
    "#         return Pi\n",
    "\n",
    "#     def epsilon_greedy_policy(self, Q, s):\n",
    "\n",
    "#         p = random.random()\n",
    "#         if p < self.epsilon:\n",
    "#             return np.random.choice(self.Num_A)\n",
    "#         else:\n",
    "#             # A = np.argmax([Q[(s, a)] for a in range(self.Num_A)])\n",
    "#             # prob = np.array([np.exp(x) for x in Q[s]])\n",
    "#             # prob = prob/(sum(prob))\n",
    "#             # A = np.random.choice(self.Num_A, p=prob)\n",
    "#             A = np.argmax(Q[s])\n",
    "#             return A\n",
    "\n",
    "#     def on_policy_monte_carlo(self, num_iter=10):\n",
    "\n",
    "#         self.epsilon = 0.9\n",
    "\n",
    "#         NUM_ITER = num_iter\n",
    "\n",
    "#         Q = collections.defaultdict(lambda: np.ones(self.Num_A)/self.Num_A)\n",
    "#         returns = collections.defaultdict(float)\n",
    "\n",
    "#         Pi = self.soft_policy()\n",
    "#         print(\"Starting Policy:, \", Pi)\n",
    "\n",
    "#         S_A_count = collections.defaultdict(int)\n",
    "\n",
    "#         rewards_per_episode = []\n",
    "#         unique_states = []\n",
    "\n",
    "#         for it in range(NUM_ITER):\n",
    "#             episode = self.generate_episode(Pi)\n",
    "#             if it % 50 == 0:\n",
    "#                 print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "#             # self.epsilon = max(self.epsilon * 0.99, 0.01)\n",
    "\n",
    "#             S_A = set([(S, A) for (S, A, _) in episode])\n",
    "\n",
    "#             for S, A in S_A:\n",
    "#                 first_idx = [\n",
    "#                     i for i, (s, a, _) in enumerate(episode) if (s == S and a == A)\n",
    "#                 ][0]\n",
    "#                 G = sum(\n",
    "#                     [\n",
    "#                         r * (self.gamma ** i)\n",
    "#                         for i, (s, a, r) in enumerate(episode[first_idx:])\n",
    "#                     ]\n",
    "#                 )\n",
    "\n",
    "#                 returns[(S, A)] += G\n",
    "#                 S_A_count[(S, A)] += 1\n",
    "#                 Q[S][A] = returns[(S, A)] / S_A_count[(S, A)]\n",
    "\n",
    "#             distinct_states = set([s for s, a in S_A])\n",
    "\n",
    "#             for s in distinct_states:\n",
    "#                 a_star = np.argmax(Q[s])\n",
    "#                 for a in range(self.Num_A):\n",
    "#                     if a == a_star:\n",
    "#                         Pi[s][a] = 1 - self.epsilon + self.epsilon / self.Num_A\n",
    "#                     else:\n",
    "#                         Pi[s][a] = self.epsilon / self.Num_A\n",
    "\n",
    "#         return Pi\n",
    "\n",
    "#     def off_policy_monte_carlo(self, num_iter=10):\n",
    "\n",
    "#         self.epsilon = 0.3\n",
    "\n",
    "#         NUM_ITER = num_iter\n",
    "\n",
    "#         Q = collections.defaultdict(lambda: np.ones(self.Num_A)/self.Num_A)\n",
    "#         C = collections.defaultdict(lambda: np.zeros(self.Num_A))\n",
    "        \n",
    "#         Mu = self.soft_policy()\n",
    "\n",
    "#         for it in range(NUM_ITER):\n",
    "#             episode = self.generate_episode(Mu)\n",
    "#             if it % 50 == 0:\n",
    "#                 print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "#             G = 0\n",
    "#             W = 1\n",
    "            \n",
    "#             state = set()\n",
    "#             for it_, info in enumerate(episode[::-1]):\n",
    "#                 S, A, R = info\n",
    "                \n",
    "#                 state.add(S)\n",
    "                \n",
    "#                 G = self.gamma * G + R\n",
    "                \n",
    "#                 C[S][A] += W\n",
    "#                 Q[S][A] = Q[S][A] + (W/C[S][A]) * (G - Q[S][A])\n",
    "\n",
    "#                 Pi = self.epsilon_greedy_policy(Q, S)\n",
    "                \n",
    "#                 W = W * Pi / Mu[S, A]\n",
    "                \n",
    "#                 if W == 0:\n",
    "#                     print(S, A, R, it_)\n",
    "#                     break\n",
    "\n",
    "#             print(state)\n",
    "#         pprint(Q)\n",
    "#         return Q\n",
    "\n",
    "#     def q_learning(self, num_iter=301):\n",
    "\n",
    "#         self.epsilon = .9\n",
    "\n",
    "#         NUM_ITER = num_iter\n",
    "\n",
    "#         Q = collections.defaultdict(lambda: np.zeros(self.Num_A))\n",
    "\n",
    "#         for it in range(NUM_ITER):\n",
    "\n",
    "#             if it % 50 == 0:\n",
    "#                 print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "#             S = self.env.reset()\n",
    "\n",
    "#             # self.epsilon = max(self.epsilon * 0.999, 0.1)\n",
    "\n",
    "#             while True:\n",
    "#                 A = self.epsilon_greedy_policy(Q, S)\n",
    "#                 S_, R, terminal, _ = self.env.step(A)\n",
    "\n",
    "#                 Q[S][A] += self.alpha * (R + self.gamma * max(Q[S_]) - Q[S][A])\n",
    "\n",
    "#                 S = S_\n",
    "\n",
    "#                 if terminal:\n",
    "#                     break\n",
    "\n",
    "#         return Q\n",
    "\n",
    "#     def sarsa(self, num_iter=301):\n",
    "\n",
    "#         self.epsilon = .9\n",
    "\n",
    "#         NUM_ITER = num_iter\n",
    "\n",
    "#         Q = collections.defaultdict(lambda: np.zeros(self.Num_A))\n",
    "\n",
    "#         for it in range(NUM_ITER):\n",
    "#             if it % 50 == 0:\n",
    "#                 print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "#             S = self.env.reset()\n",
    "#             A = self.epsilon_greedy_policy(Q, S)\n",
    "\n",
    "#             #self.epsilon = max(self.epsilon * 0.999, 0.1)\n",
    "\n",
    "#             while True:\n",
    "#                 S_, R, terminal, _ = self.env.step(A)\n",
    "#                 A_ = self.epsilon_greedy_policy(Q, S_)\n",
    "                \n",
    "#                 Q[S][A] += self.alpha*(R + (self.gamma * Q[S_][A_]) - Q[S][A])\n",
    "\n",
    "#                 S = S_\n",
    "#                 A = A_\n",
    "\n",
    "#                 if terminal:\n",
    "#                     break\n",
    "#         return Q\n",
    "\n",
    "#     def generate_episode(self, Pi):\n",
    "\n",
    "#         episode = []\n",
    "\n",
    "#         S = self.env.reset()\n",
    "#         while True:\n",
    "#             A = np.random.choice(np.arange(self.Num_A), p=Pi[S])\n",
    "#             S_, R, terminal, _ = self.env.step(A)\n",
    "#             episode.append((S, A, R))\n",
    "#             S = S_\n",
    "#             if terminal:\n",
    "#                 break\n",
    "#         return episode\n",
    "\n",
    "#     def show_policy(self, Q):\n",
    "\n",
    "#         MAX_STEPS = 500\n",
    "\n",
    "#         S = self.env.reset()\n",
    "#         print(f\"Starting state: {S}\")\n",
    "#         # self.env.render()\n",
    "\n",
    "#         step = 0\n",
    "#         while step < MAX_STEPS:\n",
    "#             # A = self.epsilon_greedy_policy(Q, S)\n",
    "#             A = np.argmax(Q[S])\n",
    "#             S_, R, done, _ = self.env.step(A)\n",
    "#             # self.env.render()\n",
    "#             if done:\n",
    "#                 break\n",
    "#             print(\n",
    "#                 f\"Current State: {S}, action: {A}, reward: {R}, done: {done}, step: {step}\"\n",
    "#             )\n",
    "#             S = S_\n",
    "#             step += 1\n",
    "#         print(\n",
    "#             f\"Current State: {S}, action: {A}, reward: {R}, done: {done}, step: {step}\"\n",
    "#         )\n",
    "#         # self.env.render()\n",
    "#         self.env.close()\n",
    "#         print(\"Finished\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, environment=\"Taxi-v3\"\n",
    "    ) -> None:\n",
    "\n",
    "        self.environment = environment\n",
    "\n",
    "    def epsilon_greedy_policy(self, Pi, epsilon):\n",
    "        \n",
    "        Pi = np.around(Pi, decimals=1)\n",
    "\n",
    "        p = np.random.rand()\n",
    "        Num_A = len(Pi)\n",
    "        if p < epsilon:\n",
    "            return np.random.choice(Num_A)\n",
    "        else:\n",
    "            a_star = np.max(Pi)\n",
    "            max_indices = [i for i in range(Num_A) if Pi[i]==a_star]\n",
    "            A = np.random.choice(max_indices)\n",
    "            return A\n",
    "    \n",
    "    def soft_policy(self, Num_A):\n",
    "        A = np.random.choice(Num_A)\n",
    "        return A\n",
    "    \n",
    "    def on_policy_monte_carlo(self, num_iter=2000, alpha=0.5,  gamma=0.6, epsilon = 0.7):\n",
    "\n",
    "        env = gym.make(self.environment)\n",
    "        Num_A = env.action_space.n\n",
    "        Num_S = env.observation_space.n\n",
    "        env.reset()\n",
    "\n",
    "        NUM_ITER = num_iter\n",
    "\n",
    "        Q = np.zeros((Num_S, Num_A))\n",
    "        C = np.zeros((Num_S, Num_A))\n",
    "        rewards = np.zeros(NUM_ITER)\n",
    "\n",
    "        Pi = np.ones((Num_S, Num_A))/Num_A\n",
    "        \n",
    "        returns = collections.defaultdict(list)\n",
    "\n",
    "        for it in range(1, NUM_ITER + 1):\n",
    "            if it % 100 == 0:\n",
    "                print(f\"Generating Episode Number: {it}\")\n",
    "                \n",
    "            S = env.reset()\n",
    "            \n",
    "            G = 0\n",
    "            step = 0\n",
    "            episode = []\n",
    "            while True:\n",
    "                if np.random.rand() < epsilon:\n",
    "                    A = np.random.choice(Num_A)\n",
    "                else:\n",
    "                    a_star = np.max(Pi[S])\n",
    "                    max_indices = [i for i in range(Num_A) if Pi[S, i]==a_star]\n",
    "                    A = np.random.choice(max_indices)\n",
    "            \n",
    "                S_, R, terminal, _ = env.step(A)\n",
    "                \n",
    "                if terminal and self.environment == 'CliffWalking-v0':\n",
    "                    R = 0\n",
    "                \n",
    "                episode.append((S, A, R))\n",
    "                \n",
    "                G = G + (gamma**step)*R\n",
    "\n",
    "                S = S_\n",
    "                \n",
    "                step+=1\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "                    \n",
    "            rewards[it - 1] = G\n",
    "\n",
    "            S_A = [(S, A) for (S, A, _) in episode]\n",
    "            \n",
    "            G = 0\n",
    "            t = len(episode) - 1\n",
    "            while t >= 0:\n",
    "                S, A, R = episode[t]\n",
    "                G = G + R\n",
    "                if not (S, A) in S_A[:t]:\n",
    "                    returns[(S, A)].append(G)\n",
    "                    Q[S, A] = sum(returns[(S, A)])/len(returns[(S, A)])\n",
    "                    \n",
    "                    q_max = np.max(Q[S])\n",
    "                    max_indices = [i for i in range(Num_A) if Q[S, i]==q_max]\n",
    "                    A_star = np.random.choice(max_indices)\n",
    "                    \n",
    "                    for a in range(Num_A):\n",
    "                        if a == A_star:\n",
    "                            Pi[S, a] = 1 - epsilon + (epsilon/Num_A)\n",
    "                        else:\n",
    "                            Pi[S, a] = (epsilon/Num_A)\n",
    "                t-=1\n",
    "                \n",
    "        return Q, rewards, Pi\n",
    "    \n",
    "    def off_policy_monte_carlo(self, num_iter=500, alpha=0.1, gamma=0.9, epsilon = 0.2):\n",
    "\n",
    "        env = gym.make(self.environment)\n",
    "        Num_A = env.action_space.n\n",
    "        Num_S = env.observation_space.n\n",
    "        env.reset()\n",
    "\n",
    "        NUM_ITER = num_iter\n",
    "\n",
    "        # Q = np.random.rand(Num_S, Num_A)\n",
    "        Q = np.zeros((Num_S, Num_A))\n",
    "        C = np.zeros((Num_S, Num_A))\n",
    "        rewards = np.zeros(NUM_ITER)\n",
    "        \n",
    "        Pi = np.ones((Num_S, Num_A))/Num_A\n",
    "\n",
    "        for it in range(1, NUM_ITER + 1):\n",
    "            if it % 100 == 0:\n",
    "                print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "            S = env.reset()\n",
    "            \n",
    "            G = 0\n",
    "            step = 0\n",
    "            episode = []\n",
    "            while True:    \n",
    "                A = np.random.choice(Num_A)\n",
    "                S_, R, terminal, _ = env.step(A)\n",
    "                \n",
    "                if terminal and self.environment == 'CliffWalking-v0':\n",
    "                    R = 0\n",
    "                \n",
    "                episode.append((S, A, R))\n",
    "                \n",
    "                G = G + (gamma**step)*R\n",
    "\n",
    "                S = S_\n",
    "                \n",
    "                step+=1\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "                    \n",
    "            rewards[it - 1] = G\n",
    "            \n",
    "            G = 0\n",
    "            W = 1\n",
    "            t = len(episode) - 1\n",
    "            while t >= 0:\n",
    "                S, A, R = episode[t]\n",
    "                \n",
    "                G = gamma * G + R\n",
    "                \n",
    "                C[S, A] += W\n",
    "                Q[S, A] += (W/C[S, A]) * (G - Q[S, A])\n",
    "                \n",
    "                q_max = np.max(Q[S])\n",
    "                max_indices = [i for i in range(Num_A) if Q[S, i]==q_max]\n",
    "                A_star = np.random.choice(max_indices)\n",
    "                \n",
    "                for a in range(Num_A):\n",
    "                    if a == A_star:\n",
    "                        Pi[S, a] = 1 - epsilon + (epsilon/Num_A)\n",
    "                    else:\n",
    "                        Pi[S, a] = (epsilon/Num_A)\n",
    "                \n",
    "                W = W*(Pi[S, A]*Num_A)\n",
    "                \n",
    "                if W == 0:\n",
    "                    break\n",
    "                \n",
    "                t -= 1\n",
    "\n",
    "        return Q, rewards\n",
    "    \n",
    "    def q_learning(self, num_iter=10000, alpha=0.1, gamma=0.6, epsilon = 0.3):\n",
    "\n",
    "        env = gym.make(self.environment)\n",
    "        Num_A = env.action_space.n\n",
    "        Num_S = env.observation_space.n\n",
    "        env.reset()\n",
    "\n",
    "        NUM_ITER = num_iter\n",
    "\n",
    "        Q = np.zeros((Num_S, Num_A))\n",
    "        rewards = np.zeros(NUM_ITER)\n",
    "\n",
    "        for it in range(1, NUM_ITER + 1):\n",
    "\n",
    "            if it % 5000 == 0:\n",
    "                print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "            S = env.reset()\n",
    "            \n",
    "            G = 0\n",
    "            step = 0\n",
    "            while True:\n",
    "                A = self.epsilon_greedy_policy(Q[S], epsilon)\n",
    "                S_, R, terminal, _ = env.step(A)\n",
    "                \n",
    "                G = G + (gamma**step)*R\n",
    "\n",
    "                Q[S][A] += alpha * (R + gamma * np.max(Q[S_]) - Q[S][A])\n",
    "\n",
    "                S = S_\n",
    "                \n",
    "                step+=1\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "                    \n",
    "            rewards[it -1] = G\n",
    "\n",
    "        return Q, rewards\n",
    "\n",
    "    def sarsa(self, num_iter=10000, alpha=0.1, gamma=0.9, epsilon = 0.05):\n",
    "\n",
    "        env = gym.make(self.environment)\n",
    "        Num_A = env.action_space.n\n",
    "        Num_S = env.observation_space.n\n",
    "        env.reset()\n",
    "\n",
    "        NUM_ITER = num_iter\n",
    "\n",
    "        Q = np.zeros((Num_S, Num_A))\n",
    "        rewards = np.zeros(NUM_ITER)\n",
    "\n",
    "        for it in range(1, NUM_ITER + 1):\n",
    "            if it % 5000 == 0:\n",
    "                print(f\"Generating Episode Number: {it}\")\n",
    "\n",
    "            S = env.reset()\n",
    "            A = self.epsilon_greedy_policy(Q[S], epsilon)\n",
    "            \n",
    "            G = 0\n",
    "            step = 0\n",
    "            while True:\n",
    "                S_, R, terminal, _ = env.step(A)\n",
    "                A_ = self.epsilon_greedy_policy(Q[S_], epsilon)\n",
    "                \n",
    "                G = G + (gamma**step)*R\n",
    "\n",
    "                Q[S][A] = Q[S][A] + alpha * (R + gamma * Q[S_][A_] - Q[S][A])\n",
    "\n",
    "                S = S_\n",
    "                A = A_\n",
    "                \n",
    "                step += 1\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "            rewards[it -1] = G\n",
    "\n",
    "        return Q, rewards\n",
    "\n",
    "    def show_policy(self, Q):\n",
    "\n",
    "        MAX_STEPS = 51\n",
    "        \n",
    "        env = gym.make(self.environment)\n",
    "        Num_A = env.action_space.n\n",
    "        Num_S = env.observation_space.n\n",
    "\n",
    "        S = env.reset()\n",
    "        print(f\"Starting state: {S}\")\n",
    "\n",
    "        step = 0\n",
    "        done = False\n",
    "        while step < MAX_STEPS:\n",
    "            A = np.argmax(Q[S])\n",
    "            env.render()\n",
    "            S_, R, done, _ = env.step(A)\n",
    "            if done:\n",
    "                break\n",
    "            print(\n",
    "                f\"Current State: {S}, action: {A}, reward: {R}, terminal: {done}, step: {step}\"\n",
    "            )\n",
    "            S = S_\n",
    "            step += 1\n",
    "        if done:\n",
    "            print(\n",
    "                f\"Current State: {S}, action: {A}, reward: {R}, terminal: {done}, step: {step}\"\n",
    "            )\n",
    "            env.render()\n",
    "        env.close()\n",
    "        print(\"Finished\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = Agent(environment='CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q_value, rewards, policy = agent.on_policy_monte_carlo()\n",
    "agent.show_policy(q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Episode Number: 100\n",
      "Generating Episode Number: 200\n",
      "Generating Episode Number: 300\n",
      "Generating Episode Number: 400\n",
      "Generating Episode Number: 500\n",
      "Starting state: 36\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 36, action: 0, reward: -1, terminal: False, step: 0\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 24, action: 1, reward: -1, terminal: False, step: 1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 25, action: 1, reward: -1, terminal: False, step: 2\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 26, action: 1, reward: -1, terminal: False, step: 3\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 27, action: 0, reward: -1, terminal: False, step: 4\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 15, action: 0, reward: -1, terminal: False, step: 5\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 3, action: 1, reward: -1, terminal: False, step: 6\n",
      "o  o  o  o  x  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 4, action: 1, reward: -1, terminal: False, step: 7\n",
      "o  o  o  o  o  x  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 5, action: 1, reward: -1, terminal: False, step: 8\n",
      "o  o  o  o  o  o  x  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 6, action: 1, reward: -1, terminal: False, step: 9\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 7, action: 2, reward: -1, terminal: False, step: 10\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 19, action: 1, reward: -1, terminal: False, step: 11\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  x  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 20, action: 1, reward: -1, terminal: False, step: 12\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 21, action: 2, reward: -1, terminal: False, step: 13\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 33, action: 1, reward: -1, terminal: False, step: 14\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 34, action: 1, reward: -1, terminal: False, step: 15\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 35, action: 2, reward: -1, terminal: True, step: 16\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  x\n",
      "\n",
      "Finished True\n"
     ]
    }
   ],
   "source": [
    "off_policy, rewards = agent.off_policy_monte_carlo()\n",
    "agent.show_policy(off_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Episode Number: 5000\n",
      "Generating Episode Number: 10000\n",
      "Starting state: 36\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 36, action: 0, reward: -1, terminal: False, step: 0\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 24, action: 1, reward: -1, terminal: False, step: 1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 25, action: 1, reward: -1, terminal: False, step: 2\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 26, action: 1, reward: -1, terminal: False, step: 3\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 27, action: 1, reward: -1, terminal: False, step: 4\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 28, action: 1, reward: -1, terminal: False, step: 5\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  x  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 29, action: 1, reward: -1, terminal: False, step: 6\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  x  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 30, action: 1, reward: -1, terminal: False, step: 7\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 31, action: 1, reward: -1, terminal: False, step: 8\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  x  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 32, action: 1, reward: -1, terminal: False, step: 9\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 33, action: 1, reward: -1, terminal: False, step: 10\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 34, action: 1, reward: -1, terminal: False, step: 11\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 35, action: 2, reward: -1, terminal: True, step: 12\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  x\n",
      "\n",
      "Finished True\n"
     ]
    }
   ],
   "source": [
    "q_policy, rewards = agent.q_learning()\n",
    "agent.show_policy(q_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Episode Number: 5000\n",
      "Generating Episode Number: 10000\n",
      "Starting state: 36\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 36, action: 0, reward: -1, terminal: False, step: 0\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 24, action: 0, reward: -1, terminal: False, step: 1\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 12, action: 1, reward: -1, terminal: False, step: 2\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 13, action: 0, reward: -1, terminal: False, step: 3\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 1, action: 1, reward: -1, terminal: False, step: 4\n",
      "o  o  x  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 2, action: 1, reward: -1, terminal: False, step: 5\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 3, action: 1, reward: -1, terminal: False, step: 6\n",
      "o  o  o  o  x  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 4, action: 1, reward: -1, terminal: False, step: 7\n",
      "o  o  o  o  o  x  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 5, action: 1, reward: -1, terminal: False, step: 8\n",
      "o  o  o  o  o  o  x  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 6, action: 1, reward: -1, terminal: False, step: 9\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 7, action: 1, reward: -1, terminal: False, step: 10\n",
      "o  o  o  o  o  o  o  o  x  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 8, action: 1, reward: -1, terminal: False, step: 11\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 9, action: 1, reward: -1, terminal: False, step: 12\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 10, action: 2, reward: -1, terminal: False, step: 13\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 22, action: 1, reward: -1, terminal: False, step: 14\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 23, action: 2, reward: -1, terminal: False, step: 15\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "Current State: 35, action: 2, reward: -1, terminal: True, step: 16\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  x\n",
      "\n",
      "Finished True\n"
     ]
    }
   ],
   "source": [
    "sarsa_policy, rewards = agent.sarsa()\n",
    "agent.show_policy(sarsa_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
