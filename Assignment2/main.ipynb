{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5257031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Normal\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb7d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fcd1bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaPolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_space, action_space, hidden_size=128):\n",
    "        super(VanillaPolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_space, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, action_space),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_space, hidden_size=128):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_space, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_space, action_space, hidden_size=128):\n",
    "       \tsuper(GaussianPolicy, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_space, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.mean = nn.Linear(hidden_size, action_space)\n",
    "        self.log_std = nn.Linear(hidden_size, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.network(x)\n",
    "\n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, min=-2, max=20)\n",
    "        std = log_std.exp()\n",
    "\n",
    "        return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f734507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b4c0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MountainCar-v0, Acrobot-v1\n",
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "152d582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if env_name == \"MountainCar-v0\":\n",
    "    lr = 1e-3\n",
    "else:\n",
    "    lr = 5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80e7da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_episode(env, env_name, policy_network):\n",
    "#     rewards, actions, states = [], [], []\n",
    "#     action_probs = []\n",
    "\n",
    "#     S = env.reset()\n",
    "\n",
    "#     done = False\n",
    "#     episode_len = 0\n",
    "#     policy_network.eval()\n",
    "#     while not done:\n",
    "\n",
    "#         A_prob = policy_network(torch.tensor(S)[None])\n",
    "#         distribution = Categorical(A_prob)\n",
    "#         A = distribution.sample()\n",
    "\n",
    "#         S_, R, done, _ = env.step(A.item())\n",
    "#         episode_len += 1\n",
    "\n",
    "#         if env_name == \"MountainCar-v0\" and S_[0] > -0.2:\n",
    "#             R = 1\n",
    "\n",
    "#         states.append(S)\n",
    "#         actions.append(A)\n",
    "#         rewards.append(R)\n",
    "\n",
    "#         action_probs.append(A_prob[0, A.item()])\n",
    "\n",
    "#         S = S_\n",
    "#         if done:\n",
    "#             break\n",
    "#     env.close()\n",
    "#     return rewards, actions, states, action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5cba0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_discounted_rewards(rewards, gamma=0.8):\n",
    "#     G = []\n",
    "#     R = 0\n",
    "#     for r in rewards[::-1]:\n",
    "#         R = r + gamma * R\n",
    "#         G.insert(0, R)\n",
    "#     G = torch.tensor(G)\n",
    "#     G = (G - G.mean()) / G.std()\n",
    "    \n",
    "#     return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad264c",
   "metadata": {},
   "source": [
    "### Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807265aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:   50, Avg. Rewards: -500.0000\n",
      "Episode:  100, Avg. Rewards: -356.6200\n",
      "Episode:  150, Avg. Rewards: -296.0067\n",
      "Episode:  200, Avg. Rewards: -268.3200\n",
      "Episode:  250, Avg. Rewards: -289.0120\n",
      "Episode:  300, Avg. Rewards: -324.1767\n"
     ]
    }
   ],
   "source": [
    "# MountainCar-v0, Acrobot-v1\n",
    "env_name = 'Acrobot-v1'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "MAX_EPISODES = 1000\n",
    "lr_policy = 1.e-3\n",
    "# lr_value = 1e-3\n",
    "gamma = 0.5\n",
    "\n",
    "random.seed(4)\n",
    "torch.manual_seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "policy_network = VanillaPolicyNetwork(obs_space, action_space, hidden_size=512)\n",
    "\n",
    "policy_optimizer = torch.optim.Adam(policy_network.parameters(), lr=lr_policy)\n",
    "\n",
    "returns = []\n",
    "\n",
    "policy_network.train()\n",
    "# value_network.train()\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    \n",
    "    rewards, actions, states, values = [], [], [], []\n",
    "    action_probs = []\n",
    "\n",
    "    S = env.reset()\n",
    "\n",
    "    done = False\n",
    "    episode_len = 0\n",
    "    while not done:\n",
    "\n",
    "        action_prob = policy_network(torch.tensor(S)[None])\n",
    "        dist = Categorical(action_prob)\n",
    "        action = dist.sample()\n",
    "\n",
    "        S_, R, done, _ = env.step(action.item())\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_prob = log_prob.sum()\n",
    "        \n",
    "        episode_len += 1\n",
    "\n",
    "        if env_name == \"MountainCar-v0\" and S_[0] > -0.2:\n",
    "            R = 1\n",
    "\n",
    "        states.append(S)\n",
    "        actions.append(action)\n",
    "        rewards.append(R)\n",
    "        values.append(value)\n",
    "\n",
    "        action_probs.append(log_prob)\n",
    "\n",
    "        S = S_\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "    G = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        G.insert(0, R)\n",
    "    G = torch.tensor(G)\n",
    "    G = (G - G.mean()) / G.std()\n",
    "    \n",
    "    advantage = []\n",
    "    for R in G:\n",
    "        advantage.append(R)\n",
    "\n",
    "    advantage = torch.Tensor(advantage)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for log_prob, adv in zip(action_probs, advantage):\n",
    "        policy_loss.append(-log_prob*adv)\n",
    "    \n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    \n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    avg_rewards = torch.sum(rewards)\n",
    "    returns.append(avg_rewards)\n",
    "    \n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Episode: {episode+1:4d}, Avg. Rewards: {np.mean(returns):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802d58f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # MountainCar-v0, Acrobot-v1\n",
    "# env_name = 'Acrobot-v1'\n",
    "# env = gym.make(env_name)\n",
    "\n",
    "# obs_space = env.observation_space.shape[0]\n",
    "# action_space = env.action_space.n\n",
    "\n",
    "# MAX_EPISODES = 1000\n",
    "# lr = 1e-4\n",
    "# gamma = 0.5\n",
    "\n",
    "# random.seed(4)\n",
    "# torch.manual_seed(4)\n",
    "# np.random.seed(4)\n",
    "\n",
    "# policy_network = VanillaPolicyNetwork(obs_space, action_space, hidden_size=512)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(policy_network.parameters(), lr=lr)\n",
    "\n",
    "# returns = []\n",
    "# for episode in range(MAX_EPISODES):\n",
    "\n",
    "#     rewards, actions, states, action_probs = run_episode(env, env_name, policy_network)\n",
    "            \n",
    "#     policy_network.train()\n",
    "\n",
    "#     Q_t = calculate_discounted_rewards(rewards, gamma)\n",
    "    \n",
    "#     rewards = torch.tensor(rewards)\n",
    "    \n",
    "#     states = torch.tensor(states).float()\n",
    "#     actions = torch.tensor(actions)\n",
    "    \n",
    "#     probabilities = policy_network(states)\n",
    "#     distribution = Categorical(probabilities)\n",
    "    \n",
    "#     loss = torch.sum(-distribution.log_prob(actions) * Q_t)\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     avg_rewards = torch.sum(rewards)\n",
    "#     returns.append(avg_rewards)\n",
    "    \n",
    "#     if (episode+1) % 50 == 0:\n",
    "#         # print(states.shape, actions.shape, rewards.shape, Q_t.shape)\n",
    "#         print(f\"Episode: {episode+1:4d}, Avg. Rewards: {np.mean(returns):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19a9cf",
   "metadata": {},
   "source": [
    "### Policy Gradient with Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MountainCar-v0, Acrobot-v1\n",
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "MAX_EPISODES = 1000\n",
    "lr_policy = 1.e-3\n",
    "# lr_value = 1e-3\n",
    "gamma = 0.5\n",
    "\n",
    "random.seed(4)\n",
    "torch.manual_seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "policy_network = VanillaPolicyNetwork(obs_space, action_space, hidden_size=512)\n",
    "\n",
    "policy_optimizer = torch.optim.Adam(policy_network.parameters(), lr=lr_policy)\n",
    "\n",
    "returns = []\n",
    "\n",
    "policy_network.train()\n",
    "# value_network.train()\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    \n",
    "    rewards, actions, states, values = [], [], [], []\n",
    "    action_probs = []\n",
    "\n",
    "    S = env.reset()\n",
    "\n",
    "    done = False\n",
    "    episode_len = 0\n",
    "    while not done:\n",
    "\n",
    "        action_prob = policy_network(torch.tensor(S)[None])\n",
    "        dist = Categorical(action_prob)\n",
    "        action = dist.sample()\n",
    "\n",
    "        S_, R, done, _ = env.step(action.item())\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_prob = log_prob.sum()\n",
    "        \n",
    "        episode_len += 1\n",
    "\n",
    "        if env_name == \"MountainCar-v0\" and S_[0] > -0.2:\n",
    "            R = 1\n",
    "\n",
    "        states.append(S)\n",
    "        actions.append(action)\n",
    "        rewards.append(R)\n",
    "        values.append(value)\n",
    "\n",
    "        action_probs.append(log_prob)\n",
    "\n",
    "        S = S_\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "    G = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        G.insert(0, R)\n",
    "    G = torch.tensor(G)\n",
    "    G = (G - G.mean()) / G.std()\n",
    "    \n",
    "    advantage = []\n",
    "    for R in G:\n",
    "        advantage.append(R)\n",
    "\n",
    "    advantage = torch.Tensor(advantage)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for log_prob, adv in zip(action_probs, advantage):\n",
    "        policy_loss.append(-log_prob*adv)\n",
    "    \n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    \n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    avg_rewards = torch.sum(rewards)\n",
    "    returns.append(avg_rewards)\n",
    "    \n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Episode: {episode+1:4d}, Avg. Rewards: {np.mean(returns):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34acf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MountainCar-v0, Acrobot-v1\n",
    "# env_name = 'Acrobot-v1'\n",
    "# env = gym.make(env_name)\n",
    "\n",
    "# obs_space = env.observation_space.shape[0]\n",
    "# action_space = env.action_space.n\n",
    "\n",
    "# MAX_EPISODES = 1000\n",
    "# lr = 1e-4\n",
    "# gamma = 0.5\n",
    "\n",
    "# random.seed(4)\n",
    "# torch.manual_seed(4)\n",
    "# np.random.seed(4)\n",
    "\n",
    "# policy_network = VanillaPolicyNetwork(obs_space, action_space, hidden_size=512)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(policy_network.parameters(), lr=lr)\n",
    "\n",
    "# returns = []\n",
    "# Q_t_avg = []\n",
    "# for episode in range(MAX_EPISODES):\n",
    "\n",
    "#     rewards, actions, states, action_probs = run_episode(env, env_name, policy_network)\n",
    "            \n",
    "#     policy_network.train()\n",
    "    \n",
    "#     Q_t = calculate_discounted_rewards(rewards, gamma)\n",
    "    \n",
    "#     rewards = torch.tensor(rewards)\n",
    "    \n",
    "#     states = torch.tensor(states).float()\n",
    "#     actions = torch.tensor(actions)\n",
    "    \n",
    "#     probabilities = policy_network(states)\n",
    "#     distribution = Categorical(probabilities)\n",
    "    \n",
    "#     loss = torch.sum(-distribution.log_prob(actions) * Q_t)\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     avg_rewards = torch.sum(rewards)\n",
    "#     # print(avg_rewards.shape)\n",
    "#     returns.append(avg_rewards)\n",
    "    \n",
    "#     if (episode+1) % 50 == 0:\n",
    "        \n",
    "#         # print(states.shape, actions.shape, rewards.shape, Q_t.shape)\n",
    "#         print(f\"Episode: {episode+1:4d}, Avg. Rewards: {np.mean(returns):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d2aaa",
   "metadata": {},
   "source": [
    "### Actor-Critic Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "794c8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MountainCar-v0, Acrobot-v1\n",
    "# env_name = 'MountainCar-v0'\n",
    "# env = gym.make(env_name)\n",
    "\n",
    "# obs_space = env.observation_space.shape[0]\n",
    "# action_space = env.action_space.n\n",
    "\n",
    "# MAX_EPISODES = 1000\n",
    "# lr = 1e-3\n",
    "# gamma = 0.5\n",
    "\n",
    "# random.seed(4)\n",
    "# torch.manual_seed(4)\n",
    "# np.random.seed(4)\n",
    "\n",
    "# policy_network = VanillaPolicyNetwork(obs_space, action_space, hidden_size=512)\n",
    "# value_network = ValueNetwork(obs_space, hidden_size=512)\n",
    "\n",
    "# params = list(policy_network.parameters()) + list(value_network.parameters())\n",
    "# optimizer = torch.optim.AdamW(policy_network.parameters(), lr=lr)\n",
    "\n",
    "# returns = []\n",
    "# Q_t_avg = []\n",
    "# for episode in range(MAX_EPISODES):\n",
    "\n",
    "#     rewards, actions, states, action_probs = run_episode(env, env_name, policy_network)\n",
    "            \n",
    "#     policy_network.train()\n",
    "#     value_network.train()\n",
    "    \n",
    "#     Q_t = calculate_discounted_rewards(rewards, gamma)\n",
    "    \n",
    "#     rewards = torch.tensor(rewards)\n",
    "    \n",
    "#     states = torch.tensor(states).float()\n",
    "#     actions = torch.tensor(actions)\n",
    "#     action_probs = torch.tensor(action_probs)\n",
    "    \n",
    "#     probabilities = policy_network(states)\n",
    "#     state_values = value_network(states)\n",
    "    \n",
    "#     distribution = Categorical(probabilities)\n",
    "    \n",
    "#     loss = torch.sum(-distribution.log_prob(actions) * (Q_t - state_values.detach().squeeze(1))) + F.smooth_l1_loss(state_values.squeeze(1), Q_t)\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     avg_rewards = torch.sum(rewards)\n",
    "#     returns.append(avg_rewards)\n",
    "    \n",
    "#     if (episode + 1) % 50 == 0:\n",
    "        \n",
    "#         # print(states.shape, actions.shape, rewards.shape, Q_t.shape)\n",
    "#         print(f\"Episode: {episode+1:4d}, Avg. Rewards: {np.mean(returns):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6043002d",
   "metadata": {},
   "source": [
    "### Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "704ab652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:   50, Avg. Rewards: -279.0800\n",
      "Episode:  100, Avg. Rewards: -222.6600\n",
      "Episode:  150, Avg. Rewards: -199.4333\n",
      "Episode:  200, Avg. Rewards: -186.5600\n",
      "Episode:  250, Avg. Rewards: -185.7840\n",
      "Episode:  300, Avg. Rewards: -184.9667\n",
      "Episode:  350, Avg. Rewards: -186.9057\n",
      "Episode:  400, Avg. Rewards: -183.3150\n",
      "Episode:  450, Avg. Rewards: -180.0911\n",
      "Episode:  500, Avg. Rewards: -178.7140\n",
      "Episode:  550, Avg. Rewards: -180.2746\n",
      "Episode:  600, Avg. Rewards: -185.9017\n",
      "Episode:  650, Avg. Rewards: -191.3738\n",
      "Episode:  700, Avg. Rewards: -190.5771\n",
      "Episode:  750, Avg. Rewards: -187.8427\n",
      "Episode:  800, Avg. Rewards: -186.6400\n",
      "Episode:  850, Avg. Rewards: -184.3577\n",
      "Episode:  900, Avg. Rewards: -182.0700\n",
      "Episode:  950, Avg. Rewards: -180.2579\n",
      "Episode: 1000, Avg. Rewards: -179.2480\n"
     ]
    }
   ],
   "source": [
    "# MountainCar-v0, Acrobot-v1\n",
    "env_name = 'Acrobot-v1'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "MAX_EPISODES = 1000\n",
    "lr_policy = 1e-3\n",
    "lr_value = 1e-3\n",
    "gamma = 0.5\n",
    "\n",
    "random.seed(4)\n",
    "torch.manual_seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "policy_network = VanillaPolicyNetwork(obs_space, action_space, hidden_size=512)\n",
    "value_network = ValueNetwork(obs_space, hidden_size=512)\n",
    "\n",
    "policy_optimizer = torch.optim.Adam(policy_network.parameters(), lr=lr_policy)\n",
    "value_optimizer = torch.optim.Adam(value_network.parameters(), lr=lr_value)\n",
    "\n",
    "returns = []\n",
    "\n",
    "policy_network.train()\n",
    "value_network.train()\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    \n",
    "    rewards, actions, states, values = [], [], [], []\n",
    "    action_probs = []\n",
    "\n",
    "    S = env.reset()\n",
    "\n",
    "    done = False\n",
    "    episode_len = 0\n",
    "    while not done:\n",
    "\n",
    "        action_prob = policy_network(torch.tensor(S)[None])\n",
    "        dist = Categorical(action_prob)\n",
    "        action = dist.sample()\n",
    "\n",
    "        S_, R, done, _ = env.step(action.item())\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_prob = log_prob.sum()\n",
    "        \n",
    "        episode_len += 1\n",
    "\n",
    "        if env_name == \"MountainCar-v0\" and S_[0] > -0.2:\n",
    "            R = 1\n",
    "\n",
    "        states.append(S)\n",
    "        actions.append(action)\n",
    "        rewards.append(R)\n",
    "        values.append(value)\n",
    "\n",
    "        action_probs.append(log_prob)\n",
    "\n",
    "        S = S_\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "    value_estimates = []\n",
    "    for state in states:\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        value_estimates.append(value_network(state))\n",
    "        \n",
    "    value_estimates = torch.stack(value_estimates).squeeze()\n",
    "    \n",
    "    G = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        G.insert(0, R)\n",
    "    G = torch.tensor(G)\n",
    "    \n",
    "    advantage = []\n",
    "    for value, R in zip(value_estimates, G):\n",
    "        advantage.append(R - value)\n",
    "\n",
    "    advantage = torch.Tensor(advantage)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for log_prob, adv in zip(action_probs, advantage):\n",
    "        policy_loss.append( - log_prob * adv)\n",
    "    \n",
    "    value_loss = F.mse_loss(value_estimates, G)\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "    \n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    avg_rewards = torch.sum(rewards)\n",
    "    returns.append(avg_rewards)\n",
    "    \n",
    "    if (episode + 1) % 50 == 0:\n",
    "        \n",
    "        print(f\"Episode: {episode+1:4d}, Avg. Rewards: {np.mean(returns):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91695916",
   "metadata": {},
   "source": [
    "### Gaussian Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bfd0cf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:   50, Avg. Rewards: -4.3610\n",
      "Episode:  100, Avg. Rewards: -3.1189\n",
      "Episode:  150, Avg. Rewards: -2.6732\n",
      "Episode:  200, Avg. Rewards: -2.4493\n",
      "Episode:  250, Avg. Rewards: -2.3181\n",
      "Episode:  300, Avg. Rewards: -2.2327\n",
      "Episode:  350, Avg. Rewards: -2.1646\n",
      "Episode:  400, Avg. Rewards: -2.1164\n",
      "Episode:  450, Avg. Rewards: -2.0841\n",
      "Episode:  500, Avg. Rewards: -2.0598\n",
      "Episode:  550, Avg. Rewards: -2.0340\n",
      "Episode:  600, Avg. Rewards: -2.0133\n",
      "Episode:  650, Avg. Rewards: -1.9962\n",
      "Episode:  700, Avg. Rewards: -1.9817\n",
      "Episode:  750, Avg. Rewards: -1.9698\n",
      "Episode:  800, Avg. Rewards: -1.9577\n",
      "Episode:  850, Avg. Rewards: -1.9458\n",
      "Episode:  900, Avg. Rewards: -1.9355\n",
      "Episode:  950, Avg. Rewards: -1.9273\n",
      "Episode: 1000, Avg. Rewards: -1.9195\n"
     ]
    }
   ],
   "source": [
    "# MountainCar-v0, Acrobot-v1\n",
    "env_name = 'MountainCarContinuous-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space\n",
    "\n",
    "MAX_EPISODES = 1000\n",
    "lr_policy = 1e-3\n",
    "lr_value = 1e-3\n",
    "gamma = 0.5\n",
    "\n",
    "random.seed(4)\n",
    "torch.manual_seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "num_outputs = action_space.shape[0]\n",
    "\n",
    "gaussian_policy = GaussianPolicy(obs_space, num_outputs, hidden_size=512)\n",
    "value_network = ValueNetwork(obs_space, hidden_size=512)\n",
    "\n",
    "policy_optimizer = torch.optim.Adam(gaussian_policy.parameters(), lr=lr_policy)\n",
    "value_optimizer = torch.optim.Adam(value_network.parameters(), lr=lr_value)\n",
    "\n",
    "returns = []\n",
    "\n",
    "gaussian_policy.train()\n",
    "value_network.train()\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    \n",
    "    rewards, actions, states, values = [], [], [], []\n",
    "    action_probs = []\n",
    "\n",
    "    S = env.reset()\n",
    "\n",
    "    done = False\n",
    "    episode_len = 0\n",
    "    while not done:\n",
    "\n",
    "        mu, std = gaussian_policy(torch.tensor(S)[None])\n",
    "        # value = value_network(torch.tensor(S)[None])\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_prob = log_prob.sum()\n",
    "        \n",
    "        action = torch.tanh(action)\n",
    "        action = action.numpy()[0]\n",
    "\n",
    "        # print(action, action.item())\n",
    "        S_, R, done, _ = env.step(action)\n",
    "        episode_len += 1\n",
    "\n",
    "        if env_name == \"MountainCar-v0\" and S_[0] > -0.2:\n",
    "            R = 1\n",
    "\n",
    "        states.append(S)\n",
    "        actions.append(action)\n",
    "        rewards.append(R)\n",
    "        values.append(value)\n",
    "\n",
    "        action_probs.append(log_prob)\n",
    "\n",
    "        S = S_\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "    value_estimates = []\n",
    "    for state in states:\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        value_estimates.append(value_network(state))\n",
    "        \n",
    "    value_estimates = torch.stack(value_estimates).squeeze()\n",
    "    \n",
    "    G = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        G.insert(0, R)\n",
    "    G = torch.tensor(G)\n",
    "    \n",
    "    advantage = []\n",
    "    for value, R in zip(value_estimates, G):\n",
    "        advantage.append(R - value)\n",
    "\n",
    "    advantage = torch.Tensor(advantage)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for log_prob, adv in zip(action_probs, advantage):\n",
    "        policy_loss.append( - log_prob * adv)\n",
    "    \n",
    "    v_loss = F.mse_loss(value_estimates, G)\n",
    "    value_optimizer.zero_grad()\n",
    "    v_loss.backward()\n",
    "    value_optimizer.step()\n",
    "    \n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    avg_rewards = torch.sum(rewards)\n",
    "    returns.append(avg_rewards)\n",
    "    \n",
    "    if (episode + 1) % 50 == 0:\n",
    "        \n",
    "        # print(states.shape, actions.shape, rewards.shape, Q_t.shape)\n",
    "        print(f\"Episode: {episode+1:4d}, Avg. Rewards: {np.mean(returns):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb8a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
